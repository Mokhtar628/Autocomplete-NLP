{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AO_jGolHfn2I",
        "outputId": "95991aaf-4f34-4854-e2a9-f0ea378387d9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Archive:  /content/drive/MyDrive/en_US.twitter.txt.zip\n",
            "  inflating: en_US.twitter.txt       \n"
          ]
        }
      ],
      "source": [
        "!unzip /content/drive/MyDrive/en_US.twitter.txt.zip"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        },
        "id": "BnXBSm0Nfu_0",
        "outputId": "35a80a0d-443d-42b6-d3ea-f4271ab8b7bd"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "\"How are you? Btw thanks for the RT. You gonna be in DC anytime soon? Love to see you. Been way, way too long.\\nWhen you meet someone special... you'll know. Your heart will beat more rapidly and you'll smile for no reason.\\nthey've decided its more fun if I don't.\\nSo Tired D; Played Lazer Tag & Ran A \""
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# load the data\n",
        "with open(\"./twitter_sm.txt\", \"r\") as f:\n",
        "    data = f.read()\n",
        "\n",
        "# print part of the data\n",
        "display(data[0:300])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oNijJHItiwFf",
        "outputId": "42bbbe63-fc67-4b08-8d3e-583fb5915edb"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[['How', 'are', 'you', '?'], ['Btw', 'thanks', 'for', 'the', 'RT', '.'], ['You', 'gon', 'na', 'be', 'in', 'DC', 'anytime', 'soon', '?'], ['Love', 'to', 'see', 'you', '.'], ['Been', 'way', ',', 'way', 'too', 'long', '.']]\n"
          ]
        }
      ],
      "source": [
        "# tokenize the data\n",
        "from nltk.tokenize import sent_tokenize, word_tokenize\n",
        "sentences = [word_tokenize(senctence) for senctence in sent_tokenize(data)]\n",
        "\n",
        "# display some sentences\n",
        "print(sentences[:5])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "id": "GNF0B0YpwV-l"
      },
      "outputs": [],
      "source": [
        "# save resulted lists \n",
        "import pickle\n",
        "\n",
        "with open(\"./sentences\", \"wb\") as fp:   #Picklin\n",
        "  pickle.dump(sentences, fp)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E3d_v9pGsTVY",
        "outputId": "dda06a01-4e13-4160-b918-f6c82324438a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[['how', 'are', 'you', '?'], ['btw', 'thanks', 'for', 'the', 'rt', '.'], ['you', 'gon', 'na', 'be', 'in', 'dc', 'anytime', 'soon', '?'], ['love', 'to', 'see', 'you', '.'], ['been', 'way', ',', 'way', 'too', 'long', '.']]\n"
          ]
        }
      ],
      "source": [
        "# preprocess the data\n",
        "## no need to remove stop words\n",
        "## no need to stem or lemmize the words\n",
        "## it is optional to remove punctuation but we chose not to do\n",
        "## it is important to lowercasing the words\n",
        "\n",
        "for i, sent in enumerate(sentences):\n",
        "  for j, word in enumerate(sent):\n",
        "    sentences[i][j] = word.lower()\n",
        "\n",
        "# display some sentences\n",
        "print(sentences[:5])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "id": "9AZQixjOpRfB"
      },
      "outputs": [],
      "source": [
        "# save resulted lists \n",
        "with open(\"./lower_sentences\", \"wb\") as fp:   #Picklin\n",
        "  pickle.dump(sentences, fp)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p0QQ2vSYxEFL",
        "outputId": "c0509f6e-1d87-418e-b27f-d8030785afbd"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{('sos',): 56910, ('how',): 1547, ('are',): 3366, ('you',): 12281, ('?',): 8502}\n",
            "{('sos', 'sos'): 56910, ('sos', 'how'): 391, ('how', 'are'): 83, ('are', 'you'): 490, ('you', '?'): 196}\n"
          ]
        }
      ],
      "source": [
        "# calculate unigram and bigram\n",
        "def n_grams_counter(data, n, start_token='', end_token = ''):\n",
        "    n_grams = {}\n",
        "\n",
        "    for sentence in data: \n",
        "        sentence = [start_token] * n + sentence + [end_token]\n",
        "        sentence = tuple(sentence)\n",
        "        \n",
        "        if n==1: #unigram\n",
        "          m = len(sentence)\n",
        "        else:\n",
        "          m = len(sentence) - 1\n",
        "\n",
        "        for i in range(m): \n",
        "            n_gram = sentence[i:i+n]\n",
        "            if n_gram in n_grams.keys(): \n",
        "                n_grams[n_gram] += 1\n",
        "            else:\n",
        "                n_grams[n_gram] = 1\n",
        "    \n",
        "    return n_grams\n",
        "\n",
        "unigram = n_grams_counter(sentences, 1, 'sos', 'eos')\n",
        "bigram = n_grams_counter(sentences, 2, 'sos', 'eos')\n",
        "\n",
        "# display some results\n",
        "print(dict(list(unigram.items())[:5]))\n",
        "print(dict(list(bigram.items())[:5]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "id": "pIYfEhAx2QOi"
      },
      "outputs": [],
      "source": [
        "# save resulted lists \n",
        "\n",
        "with open(\"./unigrams\", \"wb\") as fp:   #Picklin\n",
        "  pickle.dump(unigram, fp)\n",
        "\n",
        "with open(\"./bigrams\", \"wb\") as fp:   #Picklin\n",
        "  pickle.dump(bigram, fp)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 118,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0f4_MpH9MRrD",
        "outputId": "d2c6116b-7b6b-43d5-eaa4-34f7e57b18ef"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "746976\n",
            "41726\n"
          ]
        }
      ],
      "source": [
        "# get the vocab\n",
        "## load sentences\n",
        " \n",
        "with open(\"./lower_sentences\", \"rb\") as fp:\n",
        "  sentences = pickle.load(fp)\n",
        "\n",
        "\n",
        "vocab = []\n",
        "words = []\n",
        "for sent in sentences:\n",
        "  for word in sent:\n",
        "    words.append(word)\n",
        "print(len(words))\n",
        "vocab = list(set(words))\n",
        "print(len(vocab))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "id": "h0dJqQto_HfH"
      },
      "outputs": [],
      "source": [
        "# save resulted lists \n",
        "\n",
        "with open(\"./vocabulary\", \"wb\") as fp:   #Picklin\n",
        "  pickle.dump(vocab, fp)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "metadata": {
        "id": "ATDgkalvAHNM"
      },
      "outputs": [],
      "source": [
        "def get_propability_of_specific_word(word, current_token, unigram, bigram, vocabulary_size, k=1.0):\n",
        "    \n",
        "    current_token = tuple(current_token)\n",
        "\n",
        "    current_token_count = unigram[current_token] if current_token in unigram else 0\n",
        "    \n",
        "    denominator = current_token_count + k * vocabulary_size\n",
        "\n",
        "    n_plus1_gram = current_token + (word,)\n",
        "  \n",
        "    n_plus1_gram_count = bigram[n_plus1_gram] if n_plus1_gram in bigram else 0\n",
        "        \n",
        "    numerator = n_plus1_gram_count + k\n",
        "\n",
        "    probability = numerator / denominator\n",
        "    \n",
        "    return probability\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 141,
      "metadata": {
        "id": "laUl7la7_8Fy"
      },
      "outputs": [],
      "source": [
        "from operator import itemgetter\n",
        "def get_propability_of_all_words(current_token, unigram, bigram, vocabulary, k=1.0):\n",
        "   \n",
        "    current_token = tuple(current_token)\n",
        "    vocabulary = vocabulary + [\"\", \"\"]\n",
        "    vocabulary_size = len(vocabulary)\n",
        "    \n",
        "    probabilities = {}\n",
        "    for word in vocabulary:\n",
        "        probability = get_propability_of_specific_word(word, current_token, \n",
        "                                           unigram, bigram, \n",
        "                                           vocabulary_size, k=k)\n",
        "        probabilities[word] = probability\n",
        "    #dict(list(probabilities.items())[:3])\n",
        "    return dict(sorted(probabilities.items(), key = itemgetter(1), reverse = True)[:10])\n",
        "    # probabilities = []\n",
        "    # for word in vocabulary:\n",
        "    #     probability = get_propability_of_specific_word(word, current_token, unigram, bigram, vocabulary_size, k=k)\n",
        "    #     probabilities.append(probability)\n",
        "    # import numpy as np\n",
        "    # return np.argmax(probabilities)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 153,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e3eJKVXxPKPn",
        "outputId": "f8c628c1-fda7-4438-cbd8-1c26d721ffb9"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['clothing',\n",
              " 'gypsy',\n",
              " 'victims',\n",
              " 'hopping',\n",
              " 'antioch',\n",
              " 'beauts',\n",
              " 'pastries',\n",
              " 'mulholland',\n",
              " '1682',\n",
              " 'fiat']"
            ]
          },
          "execution_count": 153,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# # test your code\n",
        "# sents = [['i', 'like', 'a', 'cat'],\n",
        "#              ['this', 'dog', 'is', 'like', 'a', 'cat']]\n",
        "# unique_words = list(set(sents[0] + sents[1]))\n",
        "# unigram_counts = n_grams_counter(sents, 1)\n",
        "# bigram_counts = n_grams_counter(sents, 2)\n",
        "# get_propability_of_all_words(\"this\", unigram_counts, bigram_counts, unique_words, k=1)\n",
        "\n",
        "vocab[:10]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 150,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kQUP6V21C1Y8",
        "outputId": "46d60e23-c955-4348-97c4-fc52ce9636fc"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'can': 0.0010624538063562453,\n",
              " 'have': 0.000877679231337768,\n",
              " '!': 0.000877679231337768,\n",
              " 'know': 0.0007852919438285292,\n",
              " 'do': 0.0007621951219512195,\n",
              " 'r': 0.0006698078344419808,\n",
              " 'get': 0.0006236141906873614,\n",
              " '?': 0.0006236141906873614,\n",
              " 'got': 0.0005543237250554324,\n",
              " 'guys': 0.0005543237250554324}"
            ]
          },
          "execution_count": 150,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# test\n",
        "get_propability_of_all_words(\"u\", unigram, bigram, vocab, k=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 159,
      "metadata": {
        "id": "BzZj3pp2XYBT"
      },
      "outputs": [],
      "source": [
        "# try with trigram\n",
        "trigram = n_grams_counter(sentences, 3)\n",
        "\n",
        "with open(\"./trigrams\", \"wb\") as fp:   #Picklin\n",
        "  pickle.dump(trigram, fp)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 165,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3lITiBa6YUdW",
        "outputId": "51bd7a45-f7dc-4a78-cbd3-bd9c8b614f71"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'you': 0.0008374007082017418,\n",
              " 'i': 0.000406737486840846,\n",
              " 'we': 0.00019140587616039813,\n",
              " 'u': 0.0001435544071202986,\n",
              " 'evangelicals': 4.785146904009953e-05,\n",
              " 'travel': 4.785146904009953e-05,\n",
              " 'vampire': 4.785146904009953e-05,\n",
              " 'april': 4.785146904009953e-05,\n",
              " 'trannys': 4.785146904009953e-05,\n",
              " 'ya': 4.785146904009953e-05}"
            ]
          },
          "execution_count": 165,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "get_propability_of_all_words([\"how\", \"do\"], bigram, trigram, vocab, k=1)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3.9.7 ('mokhenv')",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.9.7"
    },
    "vscode": {
      "interpreter": {
        "hash": "4b87cd75e9229ea75b32929b05411870f9289d5bdc6a045a648f0048c144d195"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
